---
title: "Kubernetes-Native Data Orchestration: Building Scalable Airflow Pipelines"
description: "A comprehensive guide to deploying production-grade Apache Airflow on Kubernetes with real-world configurations and troubleshooting"
date: "2026-02-11"
author: "Data Mastery Lab"
tags: ["Kubernetes", "Apache Airflow", "Data Engineering", "DevOps", "Docker", "Helm"]
published: true
image: "/content/blog/assets_2/article_cover.png"
---

## Introduction

While **Docker** handles containerization by packaging applications with their dependencies, **Kubernetes** elevates this to orchestration at enterprise scale. Kubernetes is the de facto standard for managing containerized workloads in production, providing automated deployment, scaling, self-healing, and resource optimization across distributed systems.

### Why Kubernetes for Data Engineering?

Modern data pipelines require:
- **Dynamic resource allocation** for varying workload demands
- **Fault tolerance** with automatic recovery from failures
- **Scalability** to handle growing data volumes
- **Infrastructure abstraction** across cloud providers

Kubernetes delivers all of these through its robust architecture:

![Kubernetes Architecture](/content/blog/assets_2/Kubernetes_Architecture.png)

**Key Components:**
- **Control Plane**: Manages cluster state through the API server, scheduler, controller manager, and etcd
- **Worker Nodes**: Execute containerized workloads via kubelet and container runtime
- **Pods**: Smallest deployable units containing one or more containers
- **Services**: Enable network communication between pods
- **Persistent Volumes**: Provide durable storage for stateful applications

**If you've mastered Kubernetes fundamentals and want to deploy production-grade data orchestration platforms, this guide walks through a complete Apache Airflow implementation on Kubernetes with real-world configurations and troubleshooting.**


## Prerequisites and Environment Setup

### System Requirements
- Docker Desktop with Kubernetes support

### Enabling Kubernetes on Docker Desktop (Local Development Cluster)

Docker Desktop provides a single-node Kubernetes cluster ideal for development and testing before deploying to production environments.

1. Open Docker Desktop settings
2. Navigate to Kubernetes tab
3. Check "Enable Kubernetes"
4. Apply and restart

![Enabling Kubernetes](/content/blog/assets_2/enable%20kubernetes%20in%20docker.png)

**Note**: Allow 5-10 minutes for Kubernetes cluster initialization. Docker Desktop configures kubectl context automatically.


## Installing and Configuring kubectl (Cluster Control Interface)

kubectl is the command-line interface for interacting with Kubernetes clusters. It communicates with the Kubernetes API server to deploy applications, inspect resources, and manage cluster operations.

**Note**: This guide covers Windows installation. For other platforms (Linux, macOS, package managers), refer to the [official Kubernetes documentation](https://kubernetes.io/docs/tasks/tools/) for platform-specific installation methods.

### Installation on Windows

Download the kubectl binary:

```powershell
curl.exe -LO "https://dl.k8s.io/release/v1.28.0/bin/windows/amd64/kubectl.exe"
```

Verify checksum:

```powershell
certutil -hashfile kubectl.exe SHA256
type kubectl.exe.sha256
```

Verify installation:

```bash
kubectl version --client
```

![kubectl Installation](/content/blog/assets_2/curl%20kubectl%20then%20certutil%20-hashfile%20then%20type%20kubectl.exe.sha256%20then%20kubectl%20version%20client%20either%20normal%20or%20using%20--output=yaml.png)

![Checking kubectl Version](/content/blog/assets_2/checking%20kubectl%20version%20--client.png)

### Essential kubectl Commands

```bash
# View cluster info
kubectl cluster-info

# List all pods
kubectl get pods -A

# Check node status
kubectl get nodes

# Watch pod status in real-time
kubectl get pods -n <namespace> -w

# View pod logs
kubectl logs -n <namespace> <pod-name>

# Restart deployment
kubectl rollout restart deployment -n <namespace> <deployment-name>
```


## Setting Up Helm (Package Manager for Kubernetes)

Helm simplifies Kubernetes application deployment by packaging resources into charts. Instead of managing dozens of YAML manifests manually, Helm charts provide templated, versioned, and reusable deployments with configurable values.

### Installation

Download Helm from https://helm.sh/docs/intro/install/ and verify:

```bash
helm version
```

![Helm Version](/content/blog/assets_2/helm%20version.png)

### Adding Airflow Repository

```bash
helm repo add apache-airflow https://airflow.apache.org
helm repo update
```

![Helm Repository Setup](/content/blog/assets_2/helm%20repo%20add%20apache%20airflow,%20helm%20repo%20update,%20helm%20install%20airflow%20flow.png)


## Deploying Kubernetes Dashboard (Visual Cluster Management)

The Kubernetes Dashboard provides a web-based interface for monitoring cluster health, viewing resource utilization, debugging deployments, and managing workloads without command-line tools. Essential for visualizing pod status, logs, and events.

### Deployment

Apply the official dashboard manifests:

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
```

![Applying Dashboard Manifest](/content/blog/assets_2/kubectl%20apply%20-f%20github%20repo%20of%20the%20dashboard.png)

### Configuring RBAC (Role-Based Access Control for Security)

RBAC controls who can access the dashboard and what actions they can perform. Without proper RBAC configuration, the dashboard has read-only access. We create a ServiceAccount with cluster-admin privileges for full management capabilities.

Create three YAML files for access control:

**dashboard-adminuser.yaml**
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
```

**dashboard-clusterrole.yaml**
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
```

**dashboard-secret.yaml**
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
  annotations:
    kubernetes.io/service-account.name: "admin-user"
type: kubernetes.io/service-account-token
```

Apply configurations:

```bash
kubectl apply -f dashboard-adminuser.yaml
kubectl apply -f dashboard-clusterrole.yaml
kubectl apply -f dashboard-secret.yaml
```
![Applying Dashboard RBAC](/content/blog/assets_2/adminuser.png)
![Applying Dashboard RBAC](/content/blog/assets_2/kubectl%20apply%20-f%20dashboard%20clusterrole%20%20dashboard%20secret%20.yaml.png)


### Accessing the Dashboard

Start kubectl proxy:

```bash
kubectl proxy
```

![kubectl Proxy](/content/blog/assets_2/kubectl%20proxy%20to%20access%20the%20dashboard.png)

Access at:
```
http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login
```

![kubernetes Dashboard Access ](/content/blog/assets_2/access.png)

### Generating Access Token (Authentication for Dashboard Login)

Kubernetes uses token-based authentication for dashboard access. The token verifies your identity and permissions.

**Windows PowerShell:**

```powershell
kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath="{.data.token}" `
| ForEach-Object { [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($_)) }
```

**Alternative command:**

```bash
kubectl -n kubernetes-dashboard create token admin-user
```

![Getting Admin Token](/content/blog/assets_2/kubectl%20get%20admin-user.png)

Use the generated token to log in.

![Dashboard Overview](/content/blog/assets_2/kubernetes%20dashboard%20overview.png)


## Deploying Apache Airflow on Kubernetes (Scalable Workflow Orchestration)

Apache Airflow orchestrates complex data workflows through Directed Acyclic Graphs (DAGs). Deploying on Kubernetes provides enterprise-grade capabilities that transform Airflow into a production-ready orchestration platform:

![Kubernetes Benefits for Airflow](/content/blog/assets_2/dep_k_enables.png)

### Architecture Considerations

**Executor Options:**
- **CeleryExecutor**: Uses persistent worker pods, tasks run on dedicated workers
- **KubernetesExecutor**: Spawns ephemeral pods per task, better resource utilization

### External PostgreSQL Setup (Metadata Database Isolation)

Airflow stores DAG metadata, task states, and execution history in PostgreSQL. Running PostgreSQL externally (outside Kubernetes) simplifies troubleshooting, enables database backups independent of cluster state, and avoids image pull issues common with bitnami images.

Run PostgreSQL in Docker:

```bash
docker run -d --name postgres-airflow \
  -e POSTGRES_USER=airflow \
  -e POSTGRES_PASSWORD=airflow \
  -e POSTGRES_DB=airflow \
  -p 5432:5432 \
  postgres:15-alpine
```

### Installing Airflow with Helm

**Basic installation (if embedded PostgreSQL works):**

```bash
helm install airflow apache-airflow/airflow \
  --namespace airflow \
  --create-namespace \
  --timeout 15m \
  --debug
```

![Airflow Installation](/content/blog/assets_2/cli%20to%20install%20using%20helm%20install%20airflow%20--namespace%20airflow%20--create-namespace.png)

**Note**: If you encounter an error like "repo airflow not found", add the repository first:

```bash
helm repo add apache-airflow https://airflow.apache.org
helm repo update
```

**Installation with external PostgreSQL (Windows one-liner):**

```bash
helm install airflow apache-airflow/airflow --namespace airflow --create-namespace --timeout 15m --set postgresql.enabled=false --set data.metadataConnection.user=airflow --set data.metadataConnection.pass=airflow --set data.metadataConnection.protocol=postgresql --set data.metadataConnection.host=host.docker.internal --set data.metadataConnection.port=5432 --set data.metadataConnection.db=airflow --debug
```

### Configuration with values.yaml (Customizing Airflow Deployment)

- values.yaml allows customization of Helm chart parameters. This approach is cleaner than command-line flags and enables version control of your configuration.

Export default values:

```bash
helm show values apache-airflow/airflow > values.yaml
```

Edit values.yaml for your environment:

```yaml
fernetKey: "<your-fernet-key>"
webserverSecretKey: "<your-secret-key>" // same as fernet-key
executor: "KubernetesExecutor"

postgresql:
  enabled: false

data:
  metadataConnection:
    user: airflow
    pass: airflow
    protocol: postgresql
    host: host.docker.internal
    port: 5432
    db: airflow

dags:
  gitSync:
    enabled: true
    repo: https://github.com/yourusername/your-repo.git
    branch: main
    rev: HEAD
    depth: 1
    maxFailures: 0
    subPath: "dags"


```

**Retrieving Fernet Key (Windows):**

```powershell
$fernetKey = kubectl get secret --namespace airflow airflow-fernet-key -o jsonpath="{.data.fernet-key}"
[System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($fernetKey))
```

![Fernet Key Command](/content/blog/assets_2/cmd%20line%20to%20get%20the%20fernet%20key.png)

Apply configuration:

```bash
helm upgrade --install airflow apache-airflow/airflow \
  --namespace airflow \
  --create-namespace \
  -f values.yaml
```

![Helm Upgrade](/content/blog/assets_2/helm%20upgrade%20install%20airflow%20using%20the%20values.yaml.png)

### Monitoring Deployment (Verifying Pod Health and Readiness)

Watch pod status:

```bash
kubectl get pods -n airflow -w
```

Wait for all pods to reach Running state. You can verify the deployment status in the Kubernetes Dashboard:

![Workload Status](/content/blog/assets_2/workload%20status%20in%20kubernetes%20dashboard%20everything%20is%20in%20a%20success%20status.png)

All deployments, pods, and services should show green status indicators once the installation completes successfully.


## Accessing Airflow UI (Workflow Management Interface)

### Port Forwarding (Exposing Services Locally)

Port forwarding creates a secure tunnel from your local machine to a Kubernetes service. This avoids exposing services publicly while enabling local development and debugging.

Airflow 3.x uses API server instead of traditional webserver:

```bash
kubectl port-forward svc/airflow-api-server 8080:8080 --namespace airflow
```

![Port Forwarding](/content/blog/assets_2/port%20forward%20to%208080%20airflow%20api%20server%20in%20our%20case%20because%20webserver%20didnt%20work.png)

Access at: http://localhost:8080

### Troubleshooting

If API server fails:

```bash
kubectl rollout restart deployment -n airflow airflow-api-server
kubectl logs -n airflow -l component=api-server --tail=100
```


## Creating and Deploying DAGs (Building Data Workflows)

### Example DAG: hello_world.py

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta 

default_args = {
    'owner': 'datamasterylab.com',
    'start_date': datetime(2026, 9, 2),
    'catchup': False
}

dag = DAG(
    'hello_world',
    default_args=default_args,
    schedule=timedelta(days=1)
)

t1 = BashOperator(
    task_id='print_hello',
    bash_command='echo "Hello World"',
    dag=dag
)

t2 = BashOperator(
    task_id='print_dml',
    bash_command='echo "Data Mastery Lab"',
    dag=dag
)

t1 >> t2
```

### Example DAG: fetch_and_preview.py

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
import requests
import json
import pandas as pd 
from datetime import datetime, timedelta

def get_data(**kwargs):
    url = 'https://raw.githubusercontent.com/username/repo/main/data.csv'
    response = requests.get(url)
    
    if response.status_code == 200:
        df = pd.read_csv(url, header=None, names=['col1', 'col2', 'col3'])
        json_data = df.to_json(orient='records')
        kwargs['ti'].xcom_push(key='data', value=json_data)
    else:
        raise Exception(f"Failed to fetch data. Status code: {response.status_code}")

def preview_data(**kwargs):
    output_data = kwargs['ti'].xcom_pull(key='data', task_id='get_data')
    if output_data: 
        output_data = json.loads(output_data)
        df = pd.DataFrame(output_data)
        print(df.head())
    else: 
        raise ValueError("No data found in XCom")

default_args = {
    'owner': 'datamasterylab.com',
    'start_date': datetime(2026, 9, 2),
    'catchup': False
}

dag = DAG(
    'fetch_and_preview',
    default_args=default_args,
    schedule=timedelta(days=1)
)

get_data_task = PythonOperator(
    task_id='get_data',
    python_callable=get_data,
    dag=dag
)

preview_data_task = PythonOperator(
    task_id='preview_data',
    python_callable=preview_data,
    dag=dag 
)

get_data_task >> preview_data_task
```

### Deploying DAGs via GitSync (Version Control Integration)

GitSync automatically pulls DAG files from a Git repository, enabling:
- **Version control** for workflow definitions
- **Collaboration** through GitHub pull requests
- **Automated deployment** without manual file transfers

**Deployment Workflow:**

The complete workflow involves creating a GitHub repository, pushing your DAG files to a `dags` folder, configuring GitSync in your values.yaml, and letting Airflow automatically synchronize and execute your workflows:

![Deployment Workflow](/content/blog/assets_2/deployment_workflow.png)

The GitSync process runs continuously as a sidecar container in the scheduler and worker pods, ensuring your DAGs stay synchronized with the latest repository changes.

### Verifying DAG Execution (Debugging and Monitoring Workflows)

Check DAG parsing:

```bash
kubectl logs -n airflow -l component=dag-processor --tail=50
```

Monitor task execution:

```bash
kubectl logs -n airflow -l component=scheduler --tail=100
```

![DAG Success](/content/blog/assets_2/hello_word%20dag%20success.png)

![DAG Success Detail](/content/blog/assets_2/hello_word%20dag%20success%202.png)



### Security

- Store secrets in Kubernetes Secrets, not values.yaml
- Use RBAC to restrict namespace access
- Enable TLS for database connections
- Rotate Fernet keys regularly


## Conclusion

Through this implementation, we've gained hands-on experience with critical production skills: 
- Monitoring cluster health via the Kubernetes Dashboard, orchestrating data workflows with Apache Airflow, writing and deploying DAGs, configuring Helm deployments with values.yaml, and troubleshooting distributed systems. 

These foundational techniques form the backbone of modern data engineering infrastructure at scale.


## References

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Airflow Helm Chart](https://airflow.apache.org/docs/helm-chart/)
- [Kubernetes Documentation](https://kubernetes.io/docs/)
- [Helm Documentation](https://helm.sh/docs/)


